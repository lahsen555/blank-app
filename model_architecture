digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140652453676240 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	140652581485824 [label=AddmmBackward0]
	140652581485920 -> 140652581485824
	140652577163792 [label="fc.bias
 (1000)" fillcolor=lightblue]
	140652577163792 -> 140652581485920
	140652581485920 [label=AccumulateGrad]
	140652581482320 -> 140652581485824
	140652581482320 [label=ViewBackward0]
	140652581485776 -> 140652581482320
	140652581485776 [label=MeanBackward1]
	140652581485488 -> 140652581485776
	140652581485488 [label=ReluBackward0]
	140652581483568 -> 140652581485488
	140652581483568 [label=AddBackward0]
	140652581485392 -> 140652581483568
	140652581485392 [label=NativeBatchNormBackward0]
	140652581484192 -> 140652581485392
	140652581484192 [label=ConvolutionBackward0]
	140652581484816 -> 140652581484192
	140652581484816 [label=ReluBackward0]
	140652581477424 -> 140652581484816
	140652581477424 [label=NativeBatchNormBackward0]
	140652581485104 -> 140652581477424
	140652581485104 [label=ConvolutionBackward0]
	140652581486208 -> 140652581485104
	140652581486208 [label=ReluBackward0]
	140652581482896 -> 140652581486208
	140652581482896 [label=AddBackward0]
	140652581473776 -> 140652581482896
	140652581473776 [label=NativeBatchNormBackward0]
	140652581479680 -> 140652581473776
	140652581479680 [label=ConvolutionBackward0]
	140652581482992 -> 140652581479680
	140652581482992 [label=ReluBackward0]
	140652581486448 -> 140652581482992
	140652581486448 [label=NativeBatchNormBackward0]
	140652581482128 -> 140652581486448
	140652581482128 [label=ConvolutionBackward0]
	140652830959904 -> 140652581482128
	140652830959904 [label=ReluBackward0]
	140656675105920 -> 140652830959904
	140656675105920 [label=AddBackward0]
	140656675103520 -> 140656675105920
	140656675103520 [label=NativeBatchNormBackward0]
	140656675104576 -> 140656675103520
	140656675104576 [label=ConvolutionBackward0]
	140656675102656 -> 140656675104576
	140656675102656 [label=ReluBackward0]
	140656675098768 -> 140656675102656
	140656675098768 [label=NativeBatchNormBackward0]
	140656675104480 -> 140656675098768
	140656675104480 [label=ConvolutionBackward0]
	140656675105104 -> 140656675104480
	140656675105104 [label=ReluBackward0]
	140656675104432 -> 140656675105104
	140656675104432 [label=AddBackward0]
	140656675099488 -> 140656675104432
	140656675099488 [label=NativeBatchNormBackward0]
	140656675106352 -> 140656675099488
	140656675106352 [label=ConvolutionBackward0]
	140656675099680 -> 140656675106352
	140656675099680 [label=ReluBackward0]
	140656675098672 -> 140656675099680
	140656675098672 [label=NativeBatchNormBackward0]
	140656675100160 -> 140656675098672
	140656675100160 [label=ConvolutionBackward0]
	140656675098912 -> 140656675100160
	140656675098912 [label=ReluBackward0]
	140656675105056 -> 140656675098912
	140656675105056 [label=AddBackward0]
	140656675100352 -> 140656675105056
	140656675100352 [label=NativeBatchNormBackward0]
	140652827208880 -> 140656675100352
	140652827208880 [label=ConvolutionBackward0]
	140652827205568 -> 140652827208880
	140652827205568 [label=ReluBackward0]
	140652830785200 -> 140652827205568
	140652830785200 [label=NativeBatchNormBackward0]
	140652577419616 -> 140652830785200
	140652577419616 [label=ConvolutionBackward0]
	140656675099344 -> 140652577419616
	140656675099344 [label=ReluBackward0]
	140652577421536 -> 140656675099344
	140652577421536 [label=AddBackward0]
	140652577420672 -> 140652577421536
	140652577420672 [label=NativeBatchNormBackward0]
	140652577418704 -> 140652577420672
	140652577418704 [label=ConvolutionBackward0]
	140652577417408 -> 140652577418704
	140652577417408 [label=ReluBackward0]
	140652577416880 -> 140652577417408
	140652577416880 [label=NativeBatchNormBackward0]
	140652577416112 -> 140652577416880
	140652577416112 [label=ConvolutionBackward0]
	140652577415440 -> 140652577416112
	140652577415440 [label=ReluBackward0]
	140652577413280 -> 140652577415440
	140652577413280 [label=AddBackward0]
	140652577412896 -> 140652577413280
	140652577412896 [label=NativeBatchNormBackward0]
	140652577412368 -> 140652577412896
	140652577412368 [label=ConvolutionBackward0]
	140652577410304 -> 140652577412368
	140652577410304 [label=ReluBackward0]
	140652577408384 -> 140652577410304
	140652577408384 [label=NativeBatchNormBackward0]
	140652577407664 -> 140652577408384
	140652577407664 [label=ConvolutionBackward0]
	140652577413760 -> 140652577407664
	140652577413760 [label=ReluBackward0]
	140652577415056 -> 140652577413760
	140652577415056 [label=AddBackward0]
	140652577421104 -> 140652577415056
	140652577421104 [label=NativeBatchNormBackward0]
	140652577423072 -> 140652577421104
	140652577423072 [label=ConvolutionBackward0]
	140652577422880 -> 140652577423072
	140652577422880 [label=ReluBackward0]
	140652577421776 -> 140652577422880
	140652577421776 [label=NativeBatchNormBackward0]
	140652577407232 -> 140652577421776
	140652577407232 [label=ConvolutionBackward0]
	140652577423264 -> 140652577407232
	140652577423264 [label=MaxPool2DWithIndicesBackward0]
	140652577418752 -> 140652577423264
	140652577418752 [label=ReluBackward0]
	140652577422016 -> 140652577418752
	140652577422016 [label=NativeBatchNormBackward0]
	140652577421200 -> 140652577422016
	140652577421200 [label=ConvolutionBackward0]
	140652577413616 -> 140652577421200
	140652577611440 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	140652577611440 -> 140652577413616
	140652577413616 [label=AccumulateGrad]
	140652577418512 -> 140652577422016
	140652577611056 [label="bn1.weight
 (64)" fillcolor=lightblue]
	140652577611056 -> 140652577418512
	140652577418512 [label=AccumulateGrad]
	140652577422112 -> 140652577422016
	140652577610864 [label="bn1.bias
 (64)" fillcolor=lightblue]
	140652577610864 -> 140652577422112
	140652577422112 [label=AccumulateGrad]
	140652577419520 -> 140652577407232
	140652577611344 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140652577611344 -> 140652577419520
	140652577419520 [label=AccumulateGrad]
	140652577420816 -> 140652577421776
	140652577607024 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	140652577607024 -> 140652577420816
	140652577420816 [label=AccumulateGrad]
	140652577422064 -> 140652577421776
	140652577611632 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	140652577611632 -> 140652577422064
	140652577422064 [label=AccumulateGrad]
	140652577422160 -> 140652577423072
	140652577612016 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140652577612016 -> 140652577422160
	140652577422160 [label=AccumulateGrad]
	140652577413136 -> 140652577421104
	140652577612112 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	140652577612112 -> 140652577413136
	140652577413136 [label=AccumulateGrad]
	140652577417504 -> 140652577421104
	140652577612208 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	140652577612208 -> 140652577417504
	140652577417504 [label=AccumulateGrad]
	140652577423264 -> 140652577415056
	140652577412656 -> 140652577407664
	140652577612592 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140652577612592 -> 140652577412656
	140652577412656 [label=AccumulateGrad]
	140652577408912 -> 140652577408384
	140652577612688 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	140652577612688 -> 140652577408912
	140652577408912 [label=AccumulateGrad]
	140652577409680 -> 140652577408384
	140652577612784 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	140652577612784 -> 140652577409680
	140652577409680 [label=AccumulateGrad]
	140652577409824 -> 140652577412368
	140652577613168 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140652577613168 -> 140652577409824
	140652577409824 [label=AccumulateGrad]
	140652577411840 -> 140652577412896
	140652577613264 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	140652577613264 -> 140652577411840
	140652577411840 [label=AccumulateGrad]
	140652577412704 -> 140652577412896
	140652577613360 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	140652577613360 -> 140652577412704
	140652577412704 [label=AccumulateGrad]
	140652577413760 -> 140652577413280
	140652577415344 -> 140652577416112
	140652577614320 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140652577614320 -> 140652577415344
	140652577415344 [label=AccumulateGrad]
	140652577415680 -> 140652577416880
	140652577614416 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	140652577614416 -> 140652577415680
	140652577415680 [label=AccumulateGrad]
	140652577417744 -> 140652577416880
	140652577614512 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	140652577614512 -> 140652577417744
	140652577417744 [label=AccumulateGrad]
	140652577418320 -> 140652577418704
	140652577614896 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140652577614896 -> 140652577418320
	140652577418320 [label=AccumulateGrad]
	140652577412032 -> 140652577420672
	140652577614992 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	140652577614992 -> 140652577412032
	140652577412032 [label=AccumulateGrad]
	140652577411552 -> 140652577420672
	140652577615088 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	140652577615088 -> 140652577411552
	140652577411552 [label=AccumulateGrad]
	140652577412512 -> 140652577421536
	140652577412512 [label=NativeBatchNormBackward0]
	140652577421632 -> 140652577412512
	140652577421632 [label=ConvolutionBackward0]
	140652577415440 -> 140652577421632
	140652577414288 -> 140652577421632
	140652577613744 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	140652577613744 -> 140652577414288
	140652577414288 [label=AccumulateGrad]
	140652577418368 -> 140652577412512
	140652577613840 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	140652577613840 -> 140652577418368
	140652577418368 [label=AccumulateGrad]
	140652577419088 -> 140652577412512
	140652577613936 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	140652577613936 -> 140652577419088
	140652577419088 [label=AccumulateGrad]
	140652577422496 -> 140652577419616
	140652577615472 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140652577615472 -> 140652577422496
	140652577422496 [label=AccumulateGrad]
	140652577420240 -> 140652830785200
	140652577615568 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	140652577615568 -> 140652577420240
	140652577420240 [label=AccumulateGrad]
	140652577422736 -> 140652830785200
	140652577615664 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	140652577615664 -> 140652577422736
	140652577422736 [label=AccumulateGrad]
	140652827205856 -> 140652827208880
	140652577616048 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140652577616048 -> 140652827205856
	140652827205856 [label=AccumulateGrad]
	140652827205712 -> 140656675100352
	140652577616144 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	140652577616144 -> 140652827205712
	140652827205712 [label=AccumulateGrad]
	140652827208832 -> 140656675100352
	140652577616240 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	140652577616240 -> 140652827208832
	140652827208832 [label=AccumulateGrad]
	140656675099344 -> 140656675105056
	140656675103424 -> 140656675100160
	140652577617200 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140652577617200 -> 140656675103424
	140656675103424 [label=AccumulateGrad]
	140656675098240 -> 140656675098672
	140652577617296 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	140652577617296 -> 140656675098240
	140656675098240 [label=AccumulateGrad]
	140656675105248 -> 140656675098672
	140652577617392 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	140652577617392 -> 140656675105248
	140656675105248 [label=AccumulateGrad]
	140656675105296 -> 140656675106352
	140652577617776 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140652577617776 -> 140656675105296
	140656675105296 [label=AccumulateGrad]
	140656675098720 -> 140656675099488
	140652577617872 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	140652577617872 -> 140656675098720
	140656675098720 [label=AccumulateGrad]
	140656675100016 -> 140656675099488
	140652577617968 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	140652577617968 -> 140656675100016
	140656675100016 [label=AccumulateGrad]
	140656675099632 -> 140656675104432
	140656675099632 [label=NativeBatchNormBackward0]
	140656675100688 -> 140656675099632
	140656675100688 [label=ConvolutionBackward0]
	140656675098912 -> 140656675100688
	140656675105488 -> 140656675100688
	140652577616624 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140652577616624 -> 140656675105488
	140656675105488 [label=AccumulateGrad]
	140656675098816 -> 140656675099632
	140652577616720 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140652577616720 -> 140656675098816
	140656675098816 [label=AccumulateGrad]
	140656675104912 -> 140656675099632
	140652577616816 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140652577616816 -> 140656675104912
	140656675104912 [label=AccumulateGrad]
	140656675100448 -> 140656675104480
	140652577618352 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140652577618352 -> 140656675100448
	140656675100448 [label=AccumulateGrad]
	140656675105152 -> 140656675098768
	140652577618448 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	140652577618448 -> 140656675105152
	140656675105152 [label=AccumulateGrad]
	140656675100976 -> 140656675098768
	140652577618544 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	140652577618544 -> 140656675100976
	140656675100976 [label=AccumulateGrad]
	140656675099008 -> 140656675104576
	140652577618928 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140652577618928 -> 140656675099008
	140656675099008 [label=AccumulateGrad]
	140656675098432 -> 140656675103520
	140652577619024 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	140652577619024 -> 140656675098432
	140656675098432 [label=AccumulateGrad]
	140656675104528 -> 140656675103520
	140652577619120 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	140652577619120 -> 140656675104528
	140656675104528 [label=AccumulateGrad]
	140656675105104 -> 140656675105920
	140656675104288 -> 140652581482128
	140652577161392 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140652577161392 -> 140656675104288
	140656675104288 [label=AccumulateGrad]
	140652581483376 -> 140652581486448
	140652577161488 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	140652577161488 -> 140652581483376
	140652581483376 [label=AccumulateGrad]
	140652581483520 -> 140652581486448
	140652577161584 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	140652577161584 -> 140652581483520
	140652581483520 [label=AccumulateGrad]
	140652581483952 -> 140652581479680
	140652577161968 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140652577161968 -> 140652581483952
	140652581483952 [label=AccumulateGrad]
	140652581485296 -> 140652581473776
	140652577162064 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	140652577162064 -> 140652581485296
	140652581485296 [label=AccumulateGrad]
	140652581485056 -> 140652581473776
	140652577162160 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	140652577162160 -> 140652581485056
	140652581485056 [label=AccumulateGrad]
	140652581484336 -> 140652581482896
	140652581484336 [label=NativeBatchNormBackward0]
	140652830963552 -> 140652581484336
	140652830963552 [label=ConvolutionBackward0]
	140652830959904 -> 140652830963552
	140652827208784 -> 140652830963552
	140652577619504 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140652577619504 -> 140652827208784
	140652827208784 [label=AccumulateGrad]
	140652581485152 -> 140652581484336
	140652577619600 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140652577619600 -> 140652581485152
	140652581485152 [label=AccumulateGrad]
	140652581484240 -> 140652581484336
	140652577619696 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140652577619696 -> 140652581484240
	140652581484240 [label=AccumulateGrad]
	140652581485008 -> 140652581485104
	140652577162544 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140652577162544 -> 140652581485008
	140652581485008 [label=AccumulateGrad]
	140652581479584 -> 140652581477424
	140652577162640 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	140652577162640 -> 140652581479584
	140652581479584 [label=AccumulateGrad]
	140652581478384 -> 140652581477424
	140652577162736 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	140652577162736 -> 140652581478384
	140652581478384 [label=AccumulateGrad]
	140652581484288 -> 140652581484192
	140652577163120 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140652577163120 -> 140652581484288
	140652581484288 [label=AccumulateGrad]
	140652581482272 -> 140652581485392
	140652577163216 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	140652577163216 -> 140652581482272
	140652581482272 [label=AccumulateGrad]
	140652581482944 -> 140652581485392
	140652577163312 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	140652577163312 -> 140652581482944
	140652581482944 [label=AccumulateGrad]
	140652581486208 -> 140652581483568
	140652581486256 -> 140652581485824
	140652581486256 [label=TBackward0]
	140652581484960 -> 140652581486256
	140652577163696 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	140652577163696 -> 140652581484960
	140652581484960 [label=AccumulateGrad]
	140652581485824 -> 140652453676240
}
