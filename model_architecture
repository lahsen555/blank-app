digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140652458342640 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	140652826376080 [label=AddmmBackward0]
	140652826375888 -> 140652826376080
	140652577163792 [label="fc.bias
 (1000)" fillcolor=lightblue]
	140652577163792 -> 140652826375888
	140652826375888 [label=AccumulateGrad]
	140652826372384 -> 140652826376080
	140652826372384 [label=ViewBackward0]
	140652826375840 -> 140652826372384
	140652826375840 [label=MeanBackward1]
	140652826372144 -> 140652826375840
	140652826372144 [label=ReluBackward0]
	140652577348080 -> 140652826372144
	140652577348080 [label=AddBackward0]
	140652577347360 -> 140652577348080
	140652577347360 [label=NativeBatchNormBackward0]
	140652577349520 -> 140652577347360
	140652577349520 [label=ConvolutionBackward0]
	140652577350240 -> 140652577349520
	140652577350240 [label=ReluBackward0]
	140652577350960 -> 140652577350240
	140652577350960 [label=NativeBatchNormBackward0]
	140652577351728 -> 140652577350960
	140652577351728 [label=ConvolutionBackward0]
	140652577347408 -> 140652577351728
	140652577347408 [label=ReluBackward0]
	140652577353504 -> 140652577347408
	140652577353504 [label=AddBackward0]
	140652577354224 -> 140652577353504
	140652577354224 [label=NativeBatchNormBackward0]
	140652577355712 -> 140652577354224
	140652577355712 [label=ConvolutionBackward0]
	140652577356960 -> 140652577355712
	140652577356960 [label=ReluBackward0]
	140652577357440 -> 140652577356960
	140652577357440 [label=NativeBatchNormBackward0]
	140652577352880 -> 140652577357440
	140652577352880 [label=ConvolutionBackward0]
	140652577341792 -> 140652577352880
	140652577341792 [label=ReluBackward0]
	140652577342944 -> 140652577341792
	140652577342944 [label=AddBackward0]
	140652577343472 -> 140652577342944
	140652577343472 [label=NativeBatchNormBackward0]
	140652577343856 -> 140652577343472
	140652577343856 [label=ConvolutionBackward0]
	140652577344912 -> 140652577343856
	140652577344912 [label=ReluBackward0]
	140652577346112 -> 140652577344912
	140652577346112 [label=NativeBatchNormBackward0]
	140652577346640 -> 140652577346112
	140652577346640 [label=ConvolutionBackward0]
	140652577342800 -> 140652577346640
	140652577342800 [label=ReluBackward0]
	140652577347024 -> 140652577342800
	140652577347024 [label=AddBackward0]
	140652577347264 -> 140652577347024
	140652577347264 [label=NativeBatchNormBackward0]
	140652577347648 -> 140652577347264
	140652577347648 [label=ConvolutionBackward0]
	140652577347696 -> 140652577347648
	140652577347696 [label=ReluBackward0]
	140652577348176 -> 140652577347696
	140652577348176 [label=NativeBatchNormBackward0]
	140652577347936 -> 140652577348176
	140652577347936 [label=ConvolutionBackward0]
	140652577348368 -> 140652577347936
	140652577348368 [label=ReluBackward0]
	140652577348752 -> 140652577348368
	140652577348752 [label=AddBackward0]
	140652577348320 -> 140652577348752
	140652577348320 [label=NativeBatchNormBackward0]
	140652577348704 -> 140652577348320
	140652577348704 [label=ConvolutionBackward0]
	140652577349328 -> 140652577348704
	140652577349328 [label=ReluBackward0]
	140652577349136 -> 140652577349328
	140652577349136 [label=NativeBatchNormBackward0]
	140652577349616 -> 140652577349136
	140652577349616 [label=ConvolutionBackward0]
	140652577348656 -> 140652577349616
	140652577348656 [label=ReluBackward0]
	140652577349472 -> 140652577348656
	140652577349472 [label=AddBackward0]
	140652577349952 -> 140652577349472
	140652577349952 [label=NativeBatchNormBackward0]
	140652577350384 -> 140652577349952
	140652577350384 [label=ConvolutionBackward0]
	140652577350432 -> 140652577350384
	140652577350432 [label=ReluBackward0]
	140652577350816 -> 140652577350432
	140652577350816 [label=NativeBatchNormBackward0]
	140652577350768 -> 140652577350816
	140652577350768 [label=ConvolutionBackward0]
	140652577351056 -> 140652577350768
	140652577351056 [label=ReluBackward0]
	140652577351344 -> 140652577351056
	140652577351344 [label=AddBackward0]
	140652577351392 -> 140652577351344
	140652577351392 [label=NativeBatchNormBackward0]
	140652577351488 -> 140652577351392
	140652577351488 [label=ConvolutionBackward0]
	140652577351824 -> 140652577351488
	140652577351824 [label=ReluBackward0]
	140652577352304 -> 140652577351824
	140652577352304 [label=NativeBatchNormBackward0]
	140652577352688 -> 140652577352304
	140652577352688 [label=ConvolutionBackward0]
	140652577351104 -> 140652577352688
	140652577351104 [label=ReluBackward0]
	140652577352976 -> 140652577351104
	140652577352976 [label=AddBackward0]
	140652577353120 -> 140652577352976
	140652577353120 [label=NativeBatchNormBackward0]
	140652577352928 -> 140652577353120
	140652577352928 [label=ConvolutionBackward0]
	140652577353600 -> 140652577352928
	140652577353600 [label=ReluBackward0]
	140652577354128 -> 140652577353600
	140652577354128 [label=NativeBatchNormBackward0]
	140652577353984 -> 140652577354128
	140652577353984 [label=ConvolutionBackward0]
	140652577353168 -> 140652577353984
	140652577353168 [label=MaxPool2DWithIndicesBackward0]
	140652577354560 -> 140652577353168
	140652577354560 [label=ReluBackward0]
	140652577354704 -> 140652577354560
	140652577354704 [label=NativeBatchNormBackward0]
	140652577355088 -> 140652577354704
	140652577355088 [label=ConvolutionBackward0]
	140652577355472 -> 140652577355088
	140652577611440 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	140652577611440 -> 140652577355472
	140652577355472 [label=AccumulateGrad]
	140652577354368 -> 140652577354704
	140652577611056 [label="bn1.weight
 (64)" fillcolor=lightblue]
	140652577611056 -> 140652577354368
	140652577354368 [label=AccumulateGrad]
	140652577354272 -> 140652577354704
	140652577610864 [label="bn1.bias
 (64)" fillcolor=lightblue]
	140652577610864 -> 140652577354272
	140652577354272 [label=AccumulateGrad]
	140652577354464 -> 140652577353984
	140652577611344 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140652577611344 -> 140652577354464
	140652577354464 [label=AccumulateGrad]
	140652577353792 -> 140652577354128
	140652577607024 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	140652577607024 -> 140652577353792
	140652577353792 [label=AccumulateGrad]
	140652577354032 -> 140652577354128
	140652577611632 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	140652577611632 -> 140652577354032
	140652577354032 [label=AccumulateGrad]
	140652577353408 -> 140652577352928
	140652577612016 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140652577612016 -> 140652577353408
	140652577353408 [label=AccumulateGrad]
	140652577353312 -> 140652577353120
	140652577612112 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	140652577612112 -> 140652577353312
	140652577353312 [label=AccumulateGrad]
	140652577353456 -> 140652577353120
	140652577612208 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	140652577612208 -> 140652577353456
	140652577353456 [label=AccumulateGrad]
	140652577353168 -> 140652577352976
	140652577352640 -> 140652577352688
	140652577612592 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140652577612592 -> 140652577352640
	140652577352640 [label=AccumulateGrad]
	140652577352352 -> 140652577352304
	140652577612688 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	140652577612688 -> 140652577352352
	140652577352352 [label=AccumulateGrad]
	140652577352208 -> 140652577352304
	140652577612784 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	140652577612784 -> 140652577352208
	140652577352208 [label=AccumulateGrad]
	140652577352016 -> 140652577351488
	140652577613168 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140652577613168 -> 140652577352016
	140652577352016 [label=AccumulateGrad]
	140652577351584 -> 140652577351392
	140652577613264 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	140652577613264 -> 140652577351584
	140652577351584 [label=AccumulateGrad]
	140652577351200 -> 140652577351392
	140652577613360 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	140652577613360 -> 140652577351200
	140652577351200 [label=AccumulateGrad]
	140652577351104 -> 140652577351344
	140652577351152 -> 140652577350768
	140652577614320 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140652577614320 -> 140652577351152
	140652577351152 [label=AccumulateGrad]
	140652577350720 -> 140652577350816
	140652577614416 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	140652577614416 -> 140652577350720
	140652577350720 [label=AccumulateGrad]
	140652577350528 -> 140652577350816
	140652577614512 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	140652577614512 -> 140652577350528
	140652577350528 [label=AccumulateGrad]
	140652577350192 -> 140652577350384
	140652577614896 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140652577614896 -> 140652577350192
	140652577350192 [label=AccumulateGrad]
	140652577350480 -> 140652577349952
	140652577614992 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	140652577614992 -> 140652577350480
	140652577350480 [label=AccumulateGrad]
	140652577350144 -> 140652577349952
	140652577615088 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	140652577615088 -> 140652577350144
	140652577350144 [label=AccumulateGrad]
	140652577350096 -> 140652577349472
	140652577350096 [label=NativeBatchNormBackward0]
	140652577350864 -> 140652577350096
	140652577350864 [label=ConvolutionBackward0]
	140652577351056 -> 140652577350864
	140652577351008 -> 140652577350864
	140652577613744 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	140652577613744 -> 140652577351008
	140652577351008 [label=AccumulateGrad]
	140652577350288 -> 140652577350096
	140652577613840 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	140652577613840 -> 140652577350288
	140652577350288 [label=AccumulateGrad]
	140652577350048 -> 140652577350096
	140652577613936 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	140652577613936 -> 140652577350048
	140652577350048 [label=AccumulateGrad]
	140652577349712 -> 140652577349616
	140652577615472 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140652577615472 -> 140652577349712
	140652577349712 [label=AccumulateGrad]
	140652577349424 -> 140652577349136
	140652577615568 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	140652577615568 -> 140652577349424
	140652577349424 [label=AccumulateGrad]
	140652577349040 -> 140652577349136
	140652577615664 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	140652577615664 -> 140652577349040
	140652577349040 [label=AccumulateGrad]
	140652577349088 -> 140652577348704
	140652577616048 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140652577616048 -> 140652577349088
	140652577349088 [label=AccumulateGrad]
	140652577348608 -> 140652577348320
	140652577616144 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	140652577616144 -> 140652577348608
	140652577348608 [label=AccumulateGrad]
	140652577348848 -> 140652577348320
	140652577616240 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	140652577616240 -> 140652577348848
	140652577348848 [label=AccumulateGrad]
	140652577348656 -> 140652577348752
	140652577348032 -> 140652577347936
	140652577617200 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140652577617200 -> 140652577348032
	140652577348032 [label=AccumulateGrad]
	140652577348224 -> 140652577348176
	140652577617296 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	140652577617296 -> 140652577348224
	140652577348224 [label=AccumulateGrad]
	140652577347840 -> 140652577348176
	140652577617392 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	140652577617392 -> 140652577347840
	140652577347840 [label=AccumulateGrad]
	140652577347792 -> 140652577347648
	140652577617776 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140652577617776 -> 140652577347792
	140652577347792 [label=AccumulateGrad]
	140652577347504 -> 140652577347264
	140652577617872 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	140652577617872 -> 140652577347504
	140652577347504 [label=AccumulateGrad]
	140652577346976 -> 140652577347264
	140652577617968 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	140652577617968 -> 140652577346976
	140652577346976 [label=AccumulateGrad]
	140652577347072 -> 140652577347024
	140652577347072 [label=NativeBatchNormBackward0]
	140652577348128 -> 140652577347072
	140652577348128 [label=ConvolutionBackward0]
	140652577348368 -> 140652577348128
	140652577348560 -> 140652577348128
	140652577616624 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140652577616624 -> 140652577348560
	140652577348560 [label=AccumulateGrad]
	140652577347456 -> 140652577347072
	140652577616720 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140652577616720 -> 140652577347456
	140652577347456 [label=AccumulateGrad]
	140652577347600 -> 140652577347072
	140652577616816 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140652577616816 -> 140652577347600
	140652577347600 [label=AccumulateGrad]
	140652577347168 -> 140652577346640
	140652577618352 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140652577618352 -> 140652577347168
	140652577347168 [label=AccumulateGrad]
	140652577345968 -> 140652577346112
	140652577618448 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	140652577618448 -> 140652577345968
	140652577345968 [label=AccumulateGrad]
	140652577345584 -> 140652577346112
	140652577618544 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	140652577618544 -> 140652577345584
	140652577345584 [label=AccumulateGrad]
	140652577345056 -> 140652577343856
	140652577618928 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140652577618928 -> 140652577345056
	140652577345056 [label=AccumulateGrad]
	140652577344000 -> 140652577343472
	140652577619024 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	140652577619024 -> 140652577344000
	140652577344000 [label=AccumulateGrad]
	140652577343328 -> 140652577343472
	140652577619120 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	140652577619120 -> 140652577343328
	140652577343328 [label=AccumulateGrad]
	140652577342800 -> 140652577342944
	140652577341936 -> 140652577352880
	140652577161392 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140652577161392 -> 140652577341936
	140652577341936 [label=AccumulateGrad]
	140652577353216 -> 140652577357440
	140652577161488 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	140652577161488 -> 140652577353216
	140652577353216 [label=AccumulateGrad]
	140652577357008 -> 140652577357440
	140652577161584 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	140652577161584 -> 140652577357008
	140652577357008 [label=AccumulateGrad]
	140652577356192 -> 140652577355712
	140652577161968 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140652577161968 -> 140652577356192
	140652577356192 [label=AccumulateGrad]
	140652577354896 -> 140652577354224
	140652577162064 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	140652577162064 -> 140652577354896
	140652577354896 [label=AccumulateGrad]
	140652577354848 -> 140652577354224
	140652577162160 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	140652577162160 -> 140652577354848
	140652577354848 [label=AccumulateGrad]
	140652577354416 -> 140652577353504
	140652577354416 [label=NativeBatchNormBackward0]
	140652577355136 -> 140652577354416
	140652577355136 [label=ConvolutionBackward0]
	140652577341792 -> 140652577355136
	140652577342416 -> 140652577355136
	140652577619504 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140652577619504 -> 140652577342416
	140652577342416 [label=AccumulateGrad]
	140652577356528 -> 140652577354416
	140652577619600 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140652577619600 -> 140652577356528
	140652577356528 [label=AccumulateGrad]
	140652577355328 -> 140652577354416
	140652577619696 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140652577619696 -> 140652577355328
	140652577355328 [label=AccumulateGrad]
	140652577353072 -> 140652577351728
	140652577162544 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140652577162544 -> 140652577353072
	140652577353072 [label=AccumulateGrad]
	140652577351680 -> 140652577350960
	140652577162640 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	140652577162640 -> 140652577351680
	140652577351680 [label=AccumulateGrad]
	140652577350576 -> 140652577350960
	140652577162736 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	140652577162736 -> 140652577350576
	140652577350576 [label=AccumulateGrad]
	140652577349904 -> 140652577349520
	140652577163120 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140652577163120 -> 140652577349904
	140652577349904 [label=AccumulateGrad]
	140652577348800 -> 140652577347360
	140652577163216 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	140652577163216 -> 140652577348800
	140652577348800 [label=AccumulateGrad]
	140652577348992 -> 140652577347360
	140652577163312 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	140652577163312 -> 140652577348992
	140652577348992 [label=AccumulateGrad]
	140652577347408 -> 140652577348080
	140652826373104 -> 140652826376080
	140652826373104 [label=TBackward0]
	140652826378192 -> 140652826373104
	140652577163696 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	140652577163696 -> 140652826378192
	140652826378192 [label=AccumulateGrad]
	140652826376080 -> 140652458342640
}
